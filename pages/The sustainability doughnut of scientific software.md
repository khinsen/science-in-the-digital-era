The sustainability of research software has attracted much attention over the last decade. This is the consequence of software having become an essential tool for almost any form of research today. Like other research tools, e.g. scientific instruments, software requires constant attention and effort to ensure that it continues to play its role in evolving [scientific and technological environments](The%20three%20environments%20of%20scientific%20software.md). I won't go into the details of the various challenges involved, as there is a growing literature you can easily find. A good starting point is [this 2016 report of The Knowledge Exchange](https://www.knowledge-exchange.info/event/software-sustainability).

Outside of scientific research, sustainability is also an increasingly common keyword. There are ongoing discussions on the sustainability of the global economy, but also more focused discussions on particular aspects of the economy, such as agriculture or specific branches of industry. However, there is an important difference: whereas the sustainability discussion concerning research software is about ensuring *sufficient* resources, the sustainability discussion concerning the economy is about an *excessive use* of resources that is damaging the environment in which economic activity (and life!) takes place.

Economist Kate Raworth has come up with a stunningly simple visual image of sustainability, the [doughnut economy](https://www.kateraworth.com/doughnut/), along with the equally stunningly simple insight that "A healthy economy should be designed to thrive, not grow."

<a title="DoughnutEconomics, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:Doughnut_(economic_model).jpg"><img width="512" alt="Doughnut (economic model)" src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Doughnut_%28economic_model%29.jpg/512px-Doughnut_%28economic_model%29.jpg"></a>

What I will try to do in the following is apply the doughnut idea to scientific software. I won't go into the "be strong enough to thrive" aspect because that is already being discussed extensively (see the first paragraph above). It's the "not grow" part that I want to focus on, because it has not received much attention so far.

First of all: what exactly is growth, referring to a software project? Three dimensions that immediately come to mind is more users, more contributors, and more code. But there are others: More code complexity. More functionality. More dependencies. More supported platforms. There are obvious and less obvious correlations between these dimensions.

Next, what are the environmental resources that a software project could overuse? The most obvious ones are human resources, in other words: contributors and users. Software development requires competences that are relatively rare, and scientific software is worse because it requires specialized scientific domain knowledge as well. There's a finite pool of potential contributors for each project, so there is competition among different software projects in a given domain for this finite resource, and also competition between software and research projects in the same domain. The potential user community is limited as well.

The overuse scenario for human resources is a software project growing to the point that other software in the same space can no longer thrive. This would lead first to a technical monoculture and then to an epistemic monoculture. Each piece of software implements specific scientific assumptions and models that its users cannot avoid adopting. If all researchers use the same of software for some task, there is nobody left who can identify flaws in this software, or in the assumptions it is based on. Science needs [epistemic diversity](Epistemic%20diversity.md) for the same reason that natural ecosystems need biodiversity: to increase resilience.

Another resource that software can overuse is the cognitive capacity of its users,
by becoming too complex to use or understand. [Computational irreproducibility](Computational%20reproducibility.md) is one symptom of software complexity exceeding its users' capacities. The worst scenario in a research setting is software that is simple to use but too complex to understand, because it encourages scientists to use software that they do *not* understand. Unfortunately, that scenario is not only common, but even actively encouraged by institutions that see the benefit of easier-to-use software but not the risk of scientists losing [agency](Agency.md). The recent movement to create [science gateways](Science%20gateway.md) is a good example.

The problems caused by code complexity are well known, and nobody actually *wants* to increase it. But practice shows that increasing code complexity is common in software projects, usually as the result of growth along other dimensions, in particular growth in functionality and growth in the number of contributors. Code complexity growing with the number of contributors is an illustration for [Conway's law](Conway's%20law.md): the complexity of the social organization is reflected in the complexity of the artifact.

The first victims of increasing code complexity are individual users. When code is simple, they can inspect it to understand its behavior, and they can modify it to adapt it to their needs. With more complex code, they have to rely on documentation, experimentation, help from other users (or developers), and other incomplete means of understanding. Modifying the code thus requires more effort and becomes increasingly risky. Users are losing [agency](Agency.md).

Teams can better adapt to software complexification because they have more human resources at their disposal, but the cost of using the software, in terms of time and effort spent on it, nevertheless increases. As complexity increases further, the minimum size an organizations needs to have to retain agency over the code increases as well (this is an illustration of the [Law of Requisite Variety](http://pespmc1.vub.ac.be/REQVAR.html) in Cybernetics). Organizations smaller than the agency threshold are reduced in status from artisans who master their tools to machine operators that can only choose from a list of actions defined by someone else.

I have experienced this dynamics first-hand in the scientific Python ecosystem. When I discovered Python in 1995, at version 1.3, the interpreter was a compact and well-written C program that was easy to understand. Today, just understanding the release notes in detail requires the level of effort that was sufficient for understanding the code itself in 1995. The key scientific libraries followed the same path. The first release of Numerical Python in 1996 was written by one person and was also understandable by a single motivated individual. Today, NumPy's build system alone is more complex than the whole codebase in 1996. As a consequence, the original goal of the [Matrix-SIG](https://www.python.org/community/sigs/retired/matrix-sig/), which was empowering individual scientists to write their own code to solve their scientific problems, has largely been lost. Today's Python software stack empowers Facebook to build PyTorch, but for small research teams outside of computer science, choosing Python as a platform for specialized data processing scripts is a sure recipe for suffering [software collapse](Software%20collapse.md) before the end of the research project.

There are good and bad reasons for increasing code complexity. Software engineering distinguishes between *essential* complexity, which is the complexity of the problem domain, and *incidental* complexity, which comes from architectural decisions and choice of tooling and dependencies. A good example for essential complexity is the transition from [ASCII](https://en.wikipedia.org/wiki/ASCII) via [ISO 8859](https://en.wikipedia.org/wiki/ISO/IEC_8859) to [Unicode](https://en.wikipedia.org/wiki/Unicode) for representing plain text. It has lead to an increase in code complexity, but also enlarged the range of languages that can be processed from English-only to all written languages in human history. The holy grail of software engineering is accepting essential complexity while resisting the parallel introduction of incidental complexity. So far, this hasn't been very successful. Incidental complexity always sneaks into an evolving software project, often as [technical debt](Technical%20debt.md) when short-term benefits are prioritized over long-term costs.

What this means in practice is that extending functionality, for example to cover the needs of more scientific domains, or increasing diversity in the developer community, inevitably lead to increasing code complexity and thus to a shift in the target audience of the software. Software that once was a good choice for an experimental biologist doing simple data analysis becomes a good choice for large interdisciplinary teams doing climate simulations (which may well be the goal of the developer community), but at the same time it becomes unmanageable for its original audience, a side effect that the developer community is typically not aware of.

Similar processes happen in other branches of technology. Perhaps this is even the only way in which complex technology can evolve. But in other technology domains, the simple versions stay around as long as there is an audience for them. Industry has developed sophisticated tools and techniques for mass-producing complex furniture, but hand saws and screws are still available to craftspeople and amateurs. For software, it's the [collapse](Software%20collapse.md) of the foundations of the software stack that inexorably takes simple technology away from the people who relied on it. Installing Python 1.5 and Numerical Python 16.1 on a 2023 Linux system is beyond such people's means. It requires patching both packages, and a good understanding of the security issues in Python 1.5 to avoid disasters in deployment. It's only the growth-oriented branch of the Python stack that was maintained to keep up with the changes in the lower-level software it depends on. And Python is not an exception here. Only highly focused software projects manage to thrive without growing in complexity.

I see two causes for this seemingly inevitable dynamics towards growth in complexity, though I suspect there are more. One cause is technical: we don't know yet how to structure software into composable components at all relevant scales. The [Unix philosophy](https://en.wikipedia.org/wiki/Unix_philosophy), which says that a software tool should do only one thing but do it well, with complex tasks handled by the composition of several tools, just isn't applicable to software in general with today's state of the art. The other cause is social: the idea of forking a project into a stay-simple and a grow-complex branch is not part of the strategical repertoire of Open Source communities. They see the risk of a community split, but they are blind to the risk of a community slowly but continuously alienating some of its members.

The creeping complexification of software is not limited to scientific research. It happens in all application domains, and some consider it inevitable, almost as if it were a law of nature. There are, however, [projects dedicated to simplicity](In%20search%20of%20simplicity.md), which show that simplicity is achievable if it is a high-priority goal.

So far, I have described two paths in the evolution of scientific software projects that lead to an overuse of environmental resources. An obvious question is if there is something we can do about this. Can we make software projects sustainable in the long run? I am not aware of anyone even trying, so there are clearly no empirically testable answers. My aim with this essay is to launch a collective brainstorming process on these questions.

An important observation is that both of these paths are encouraged by the values of the industrial mindset that dominated the second half of the 20th century: growth, productivity, efficiency,  competition. These values still drive organizations and individuals in science and engineering, in spite of the fact that we now recognize the harm they can do when applied to the extreme. The unsustainability of software projects is very much part of the general unsustainability of today's economy. Like the economy at large, software engineering has to shift towards an ecological mindset, adopting values such as sustainability, resilience, and [coopetition](https://en.wikipedia.org/wiki/Coopetition). A necessary first step is to review current practices and analyze where they are in conflict with these new values.

A simple example for such a current best practice is [DRY: Don't repeat yourself](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself). Factoring out common operations creates new dependency relations, and thus more code complexity. There's a benefit, but also a cost. A shift to an ecological mindset implies attributing a higher cost to techniques that increase dependencies.

Social structures need to be reviewed as well, and that is likely to be more difficult than reviewing software engineering techniques. Example: the currently dominating values in scientific Open Source projects include welcoming anyone who wishes to join the community. This attitude feeds the dynamics of increasing diversity of needs that leads to increasing code complexity. The answer is of course not to be less welcoming. The answer is being explicit about the project's scope and target audience, and in particular about limits to the complexity that the project is willing to tolerate. This should reduce the number of people who are attracted to the project because they see it as a good starting point for doing something related but different. Such people should be encouraged to start a friendly fork, rather than blow up the project's coverage. In the long run, new social structures should emerge in which friendly forks cooperate without creating interdependencies, and thus complexity. This also involves solving difficult issues that seem simple at first sight. For example names: How to name friendly forks such that references are unambiguous and yet the common heritage remains visible? Who actually owns the name (= identity) of an Open Source project?

I will end this discussion by pointing out an important connection between ensuring a safe floor and an environmentally responsible ceiling to the sustainability doughnut of scientific software: investment into the infrastructure of scientific computing. There is a clear need for stable and reliable software written by professionals as a commodity for non-technical users. This requires long-term funding and long-term governance, both of which are almost completely absent today. You can't expect an Open Source community composed of volunteer early-career scientists to maintain infrastructure as a free service. Not even with a regular injection of three-year funding for one or two developers. Such communities thrive by innovating, not by providing services for others. Software infrastructure requires institutions employing experienced software developers, governed by a consortium of stakeholders that includes senior scientists who represent the users' interests.

