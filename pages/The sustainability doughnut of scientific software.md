The sustainability of research software has attracted much attention over the last decade. This is the consequence of software having become an essential tool for almost any form of research today. Like other research tools, e.g. scientific instruments, software requires constant attention and effort to ensure that it continues to play its role in evolving [scientific and technological environments](The%20three%20environments%20of%20scientific%20software.md). I won't go into the details of the various challenges involved, as there is a growing literature you can easily find. A good starting point is [this 2016 report of The Knowledge Exchange](https://www.knowledge-exchange.info/event/software-sustainability).

Outside of scientific research, sustainability is also an increasingly common keyword. There are ongoing discussions on the sustainability of the global economy, but also more focused discussions on particular aspects of the economy, such as agriculture or specific branches of industry. However, there is an important difference: whereas the sustainability discussion concerning research software is about ensuring *sufficient* resources, the sustainability discussion concerning the economy is about an *excessive use* of resources that is damaging the environment in which economic activity (and life!) takes place.

Economist Kate Raworth has come up with a stunningly simple visual image of sustainability, the [doughnut economy](https://www.kateraworth.com/doughnut/), along with the equally stunningly simple insight that "A healthy economy should be designed to thrive, not grow."

<a title="DoughnutEconomics, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:Doughnut_(economic_model).jpg"><img width="512" alt="Doughnut (economic model)" src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Doughnut_%28economic_model%29.jpg/512px-Doughnut_%28economic_model%29.jpg"></a>

What I will try to do in the following is apply the doughnut idea to scientific software. I won't go into the "be strong enough to thrive" aspect because that is already being discussed extensively (see the first paragraph above). It's the "not grow" part that I want to focus on, because it has not received much attention so far.

First of all: what exactly is growth, referring to a software project? Three dimensions that immediately come to mind is more users, more contributors, and more code (measured in kB, lines of code, or some related metric). But there are others, less straightforward to measure but still interesting to consider: More code complexity. More functionality. More dependencies. More protocols (communication, file formats, etc.). More platforms. Many of these are correlated to some degree.

Next, what are the environmental resources that a software project could overuse? The most obvious ones are human resources, or financial resources that permit to hire human resources. In other words: contributors. Software development requires competences that are relatively rare, and scientific software is worse because it requires specialized scientific domain knowledge as well. There's a finite pool of potential contributors for each project, so there is competition among different software projects in a given domain for this finite resource, and also competition between software and research projects in the same domain.

The most concrete risk is a software project growing to the point that other software in the same space can no longer thrive. This would lead first to a technical monoculture, and then to an epistemic monoculture, because each piece of software implements specific scientific assumptions and models that its users cannot avoid adopting, although this often happens unconsciously. If all researchers use the same piece of software for some task, there will be nobody left who can identify flaws in this software, or on the assumptions it is based on. Science needs [epistemic diversity](Epistemic%20diversity.md) for the same reason that natural ecosystems need biodiversity: to increase resilience.

Another dimension along which software can grow too much is code complexity, exhausting the cognitive capacity of users or contributors. The problems caused by code complexity are well known, and nobody actually *wants* to increase it. But practice shows that increasing code complexity is common in software projects, usually as the result of growth along other dimensions, in particular growth in functionality and growth in the number of contributors (which are correlated). Code complexity growing with the number of contributors is an illustration for [Conway's law](Conway's%20law.md): the complexity of the social organization is reflected in the complexity of the artifact.

The first victims of increasing code complexity are individual users. When code is simple, they can inspect it to understand its behavior, and they can modify it to adapt it to their needs. With more complex code, they have to rely on documentation, experimentation, help from other users (or developers), and other incomplete means of understanding. Modifying the code requires more effort and becomes increasingly risky. Users are losing agency. In the extreme case of users *relying* on personal modifications, the software may become completely unusable to them.

Teams can better adapt to software complexification because they have more human resources at their disposal, but the cost of using the software, in terms of time and effort spent on it, nevertheless increases. As complexity increases further, the minimum size an organizations needs to have to retain agency over the code increases as well. Organizations smaller than the agency threshold are reduced in status from artisans who master their tools to machine operators that can only choose from a list of actions defined by someone else.

I have experienced this dynamics first-hand with the scientific Python ecosystem. When I discovered Python in 1995, at version 1.3, the interpreter was a compact and well-written C program that I could understand rapidly. Today, just understanding the release notes in detail requires the level of effort that was sufficient for understanding the code itself in 1995. The key scientific libraries followed the same path. The first release of Numerical Python in 1996 was written by one person and was also understandable by a motivated individual. Today, NumPy's build system alone is more complex than that. As a consequence, the original goal of the [Matrix-SIG](https://www.python.org/community/sigs/retired/matrix-sig/), which was empowering scientists to write their own code to solve their scientific problems, has largely been lost. Today's Python software stack empowers Facebook to build PyTorch, but for small research teams outside of computer science, choosing Python as a platform for personal data processing software is a sure recipe for suffering [software collapse](Software%20collapse.md) in the near future.

There are good and bad reasons for increasing code complexity. Software engineering distinguishes between *essential* complexity, which is the complexity of the problem domain, and *incidental* complexity, which comes from architectural decisions and choice of tooling and dependencies. A good example for essential complexity is the transition from [ASCII](https://en.wikipedia.org/wiki/ASCII) via [ISO 8859](https://en.wikipedia.org/wiki/ISO/IEC_8859) to [Unicode](https://en.wikipedia.org/wiki/Unicode) for representing plain text. It has lead to an increase in code complexity, but also enlarged the range of languages that can be processed from English-only to all written languages in human history. The holy grail of software engineering is accepting essential complexity while resisting the parallel introduction of incidental complexity, but so far, this hasn't been very successful. Incidental complexity always sneaks into an evolving software project, often as [technical debt](Technical%20debt.md) when short-term benefits are prioritized over long-term costs.

What this means in practice is that extending functionality, for example to cover the needs of more scientific domains, or increasing diversity in the developer community, inevitably lead to increasing code complexity and thus to a shift in the target audience of the software. Software that once was a good choice for an experimental biologist doing simple data analysis becomes a good choice for large interdisciplinary teams doing climate simulations, but at the same time becomes unmanageable for its original audience.

Similar processes happen in other branches of technology. Perhaps this is the only way in which complex technology can evolve. But in other technology domains, the simple versions stay around as long as there is an audience for them. Industry has developed sophisticated techniques for assembling pieces of wood into furniture, but nails and screws are still available to craftspeople and amateurs. For software, it's the [collapse](Software%20collapse.md) of the foundations of the software stack that inexorably takes simple technology away from the people who relied on it. Installing Python 1.5 and Numerical Python 16.1 on a 2023 Linux system is beyond such people's means. It requires patching both packages, and a good understanding of the security issues in Python 1.5 to avoid disasters in deployment. It's only the growth-oriented branch of the Python stack that was maintained to keep up with the changes in the lower-level software it depends on. And Python is not an exception here. Only highly focused software projects manage to thrive without growing in complexity.

I see two causes for this seemingly inevitable dynamics towards growth in complexity, though I suspect there are more. One cause is technical: we don't know yet how to structure software into composable modules at all relevant scales. The philosophy of the Unix command line, where each tool should do "only one thing but do it well", with complex tasks handled by composing several such tools, just isn't applicable to software in general with today's state of the art. This means we can't have only highly focused software projects whose artifacts are easy to compose for complex tasks. The other cause is social: the idea of forking a project into a stay-simple and a grow-complex branch is not part of the strategical repertoire of Open Source communities. They see the risk of a community split but not the risk of a community slowly but continuously alienating some of its members.

The creeping complexification of software is not limited to scientific research. It happens in all application domains, and some consider it inevitable, almost as if it were a law of nature. There are, however, [projects dedicated to simplicity](Minimalism%20in%20computing%20technology.md), which show that simplicity is achievable if it is a high-priority goal. Can we do something similar for science? I think we should at least try.
