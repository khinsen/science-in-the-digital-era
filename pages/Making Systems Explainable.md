A [recorded talk](https://www.youtube.com/watch?v=jJhfTUSDlR0) by [Oscar Niestrasz](https://en.wikipedia.org/wiki/Oscar_Nierstrasz) at the [VISSOFT 2022](https://vissoft.info/2022/) conference, demoing [Glamorous Toolkit](Glamorous%20Toolkit.md) used for explaining software systems.

The talk is about software systems from the perspective of software developers. But [computer-aided research](Computer-aided%20research.md) has very much the same problems, for the same reasons, and could benefit from the same solutions. The talk starts with a demonstration of how a simple board game can be implemented in a way that makes it inspectable from a multitude of perspectives. It doesn't take much imagination to replace that board game by a computational [scientific model](Scientific%20model.md).

Consider the debate around [CovidSim](https://en.wikipedia.org/wiki/CovidSim), the  epidemiological model for COVID-19 used in the United Kingdom to inform public policies in 2020. It was heavily criticized, first for the code not being publicly available for inspection, and then for bad software engineering practices casting doubts on its reliability. Both of these criticisms are very valid. But if CovidSim had been developed openly and following best practices of [software engineering](Software%20engineering.md) for C++ code, would this have made it [trustworthy](Trustworthy%20software.md) in the eyes of (1) fellow epidemiologists, (2) public health officials and (3) the general public? I suspect the answer is no. While the model would be straightforward to run, understanding how the various modeling choices made impact the result remains very difficult because it requires modifying the code, And that requires an in-depth understanding of how the code works. But even a perfect C++ simulation code takes an immense effort to understand for anyone but its authors. 

Now imagine the models of CovidSim implemented in Glamorous Toolkit, with lots of views into the model, examples for how to run it with various parameters, and explanatory narratives linking right to these examples and views. Like the Ludo game in this talk. Quite a different scenario.

[Open Science](Open%20Science.md) requires more than papers, data, and code being publicly available. They must be made accessible for inspection and modification, at a reasonable level of effort and technical competence. We require scientific papers to be well-written. Researchers cannot just publish copies of their hand-written notes. Why should we accept them publishing software that nobody else can understand?
