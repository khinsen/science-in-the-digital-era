As the ongoing public debates about global challenges such as climate change or the Covid pandemic have shown, people are losing trust in science. In part this is due to societal changes that extend far beyond scientific practices: there is a general loss of trust in institutions, and in particular governments. Taking one more step back, there is a loss of trust in the hierarchical decision structures of Western societies, with its authorities that rely on the opinions of experts.

But there are also changes inside the scientific community that have contributed to this loss of trust, which is shared by many researchers themselves. The pressure towards bibliometric productivity and the information technology revolution have encouraged and enabled scientists to do more research but this comes sometimes at the price of a lower level of rigor. Moreover, increasing specialization makes it more difficult for scientists to judge the reliability of the work of their colleagues that they build on. The [reproducibility crisis](Reproducibility%20crisis.md) is a good illustration: a large number of studies that were widely expected to be reproducible turned out not to be.

The Open Science movement is a reaction to this development, aiming for more trustworthy processes at all levels with a focus on increasing transparency. This push towards transparency is in conflict with notions of intellectual property that have been put in place to ensure a competitive advantage, both to scientists and to the institutions funding their research. Reducing competition is thus a key to successfully implementing Open Science, but it is limited by the fact than in a context of scarce resources (mainly jobs and project funding), some level of competition is inevitable.

So far, Open Science consists of three levels of changes. The first level is Open Access: anyone should be able to consult the original publications of scientific findings, rather than having to rely on summaries provided by expert committees or journalists. Of course, few people can actually read and understand those publications. But the circle of people who can extract information reliably from these articles is still much larger than the circle of people who could afford to consult the original literature before Open Access, in particular in the less prosperous countries of our planet.

The second level is the publication of the datasets and software that underly most of today's scientific research. In the era of ubiquitous [computer-aided research](Computer-aided%20research.md), a published summary of a study and its outcome is simply no longer sufficient. Many details of the applied methods are documented only in the code ([this story](https://physicstoday.scitation.org/do/10.1063/pt.6.1.20180822a/full/) about a seven-year battle between two teams that didn't share their code is a nice illustration), and access to the data and code permits other scientists to study the same phenomena from different perspectives. Publication of data and code thus makes science more verifiable, and by enabling complementary work, it supports the construction of the web of interrelated findings that permits consensus formation and ultimately trust.

The third level of Open Science is about laying open the decision procedures in scientific research. Which topics get explored, and by whom? Who decides which results get published? How does quality control actually work? Which biases affect any of these decisions? How do political and economic interests intervene? The ultimate goal is ensuring that scientific research, in particular when it is funded from public money, benefits society as a whole.

At this time, only the first level, Open Access, has made significant progress. Its necessity is accepted by all stakeholders, but the details of implementation remain a subject of debate. The main problem is the enormous power held by the traditional scientific publishers. In the digital era, the value they contribute to the publication process has become negligible, but they still control the names of the well-respected traditional journals. These journal names are a key element in defining scientific reputation, a tendency that has become much stronger with the introduction of bibliometry into the evaluation processes of scientific institutions. The publishers are doing their best to monetize this control, by extracting hefty publication fees from authors of Open Access publications. There are clear signs that scientific institutions are willing to move away from bibliometry-based evaluation (see e.g. the [San Francisco Declaration on Research Assessment](https://sfdora.org/)), though for now it is not clear by what it is going to be replaced.

The implementation of the second level is still in its early days. Non-publication of data and code is still the norm, and even when these crucial elements are made public, they are not included in the critical examination that a paper undergoes during peer review. For data, the [FAIR principles](https://www.go-fair.org/fair-principles/) have established criteria that are increasingly accepted, though not yet massively applied. For code, only a handful of journals perform elementary tests upon submission of a paper, such as checking for [computational reproducibility](Computational%20reproducibility.md).

There is still a long way to go towards [trustworthy software](Trustworthy%20software.md). So far, no effort at all is made to check if the computations conform to the scientific methods as described in the paper. There is not even any requirement for authors to provide readable code - we are still accepting [legally open](Legally%20open%20vs.%20effectively%20open.md) code rather than insisting on [effectively open](Legally%20open%20vs.%20effectively%20open.md) code. Lack of incentives is one reason but probably not even the major one: the state of the art in scientific software makes it nearly impossible to write code that a human reader other than the authors can understand in sufficient detail. And if a reviewer cannot be expected to understand the code, there is no hope for the peer review process to inspect the code for correctness. My work on [digital scientific notations](Digital%20scientific%20notation.md) aims at addressing this issue, but many other changes need to happen, in particular to avoid falling into the trap of [cheap complexity](Cheap%20complexity.md) all the time.

Another open problem is the reviewing process itself. Today, individual reviewers are expected to comment on all aspects of a submitted study. This is unrealistic when dealing with publications that have a dozen authors from several disciplines and deploy millions of lines of code to analyze gigabytes of input data. Such submissions must be reviewed by teams combining different specializations, one of which needs to be in scientific software engineering. Moreover, such complex reviews should happen in parallel to the research itself, rather than as a single huge task at the end.

The third level is just beginning to be explored. The most concrete experimentation is open peer review, of which many varieties have been implemented by journals (e.g. [F1000Research](https://f1000research.com/), which was one of the pioneers) and independent reviewing networks (of which [PubPeer](https://pubpeer.com/) is perhaps the best known). The main difficulty faced by open peer review is being the only decision process being opened so far. This puts reviewers at the risk of retaliation by the colleagues they criticize in their reviews, e.g. in hiring or funding decisions. In contrast, open peer review works very well in less competitive contexts, such as journals that define the role of peer review as helping authors to improve their work, rather than decide on acceptance or rejection. Examples are the [Journal of Open Source Software](https://joss.theoj.org/) and [ReScience](https://rescience.github.io/). This attitude can be seen as a step towards separating dissemination and quality control in scientific research from the decision processes about resource allocation (jobs, funding, ...), which is very much in the spirit of Open Science.

Another aspect of level three is the keywords "diversity" and "inclusion" appearing on the lists of criteria for composing committees, in an attempt to make these committees more representative of society at large. For now, diversity and inclusion efforts only address the most egregious and outwardly visible misrepresentations, such as gender disparities. More subtle though numerically very important discriminations, such as the *de facto* exclusion from science of everyone who doesn't speak English, are not even discussed yet as possible objectives.

The most important decision processes in science are those concerning resource allocation: hiring and funding. They ultimately determine which research topics are being explored, and by whom. The only initiatives that I am aware of for profoundly reforming these processes happen in the so far marginal ["decentralized science"](Decentralized%20science.md) communities. In my opinion, we need more ideas to improve resource allocation, and we must actually test them in practice, rather than limit ourselves to debating their expected benefits and downsides.
