The goal of science is to construct collectively a corpus of reliable knowledge (a term I have taken from [John Ziman](https://en.wikipedia.org/wiki/John_Ziman)'s excellent book "[Reliable Knowledge: An Exploration of the Grounds for Belief in Science](https://www.cambridge.org/us/academic/subjects/general-science/popular-science/reliable-knowledge-exploration-grounds-belief-science?format=PB)"). This requires strong quality control, which is in fact what makes the difference between science and other ways of acquiring knowledge.

In the early days of science, quality control was informal. Scientists could read and understand all publications in their field and form their own judgment, because both scientists and publications were few in number. With the enormous growth of science starting in the 1950s, peer review of publications became the cornerstone of quality control. Scientists trusted work that they had not examined themselves because they trusted the peer review system and the people who supervised it, in particular journal editors. Indirect trust, through the reputation acquired over time by journals, research institutions, and also individual researchers, became a very important aspect of figuring out which results to consider reliable. However, reputation is a reliable source of trust only if it is ultimately grounded in real expertise, i.e. scientists judging the work of others based on their own understanding of it.

Today, this traditional chain of trust does not work any more. The [reproducibility crisis](Reproducibility%20crisis.md) is perhaps the most visible symptom of scientists losing trust in peer review. The public debate on climate change illustrates that the general public is losing trust in science. Something has gone wrong.

The peer review system was set up at a time when a typical publication had one to three authors, from a single discipline. Collecting reports from two or three experts in the same discipline was then a reasonable approach to evaluating the quality of the work. All reviewers could be expected to fully understand the work, and having several reviewers would reduce the risk of problematic aspects going unnoticed. In some disciplines, e.g. theoretical physics, nothing much has changed since the 1950s, and peer review continues to work rather well.

In other disciplines, such as the life sciences or climate research, a typical publication summarizes the work of a large multi-disciplinary collaboration. A proper evaluation of such work cannot be done by any individual in a week. It takes another multi-disciplinary team, and it takes a lot more time. Moreover, it takes access to much more than a few-page summary of the work. This is something the [Open Science](Open%20Science.md) movement has recognized and improved. Publication of data and code is still not universal, and many details remain to be worked out, but it seems uncontroversial to me that this is where we need to go, and the transition has started.

However, publishing data and code is not enough to build trust in large-scale multidisciplinary research. It's not even enough to build trust in datasets and code. For each form of research output, we need techniques for acquiring expert judgment and then summarizing and relaying it to a wider audience. And perhaps even "research output" is the wrong unit of evaluation. Perhaps the only practicable way to proceed is to have independent experts follow along as a research project, or a software development project, progresses through its various phases.

Ultimately, what we need to construct is a Web of Trust, much like [the concept of the same name in cryptography](https://en.wikipedia.org/wiki/Web_of_trust). For a start, we could ask authors of a scientific paper to indicate, for each artifact that they rely on (paper, software, dataset, ...), to what degree and for what reason they consider it reliable. We could also collect such judgments outside of the publication process, for example on social media. At the very least, it would force everyone to think about the question. After a while, we'd have an interesting graph of trust relation to analyze. I expect some surprises, in particular "trust bubbles": artifacts that people trust because everybody else seems to trust them, without any grounding in qualified expert judgment.

Now is a good time to think about this. Elon Musk buying Twitter has led scientists to discover and adopt Mastodon, a social network based on an open protocol. We now have a social network that scientists know and use, and which is open to extensions. Developing the infrastructure for building a trust graph is within reach.

If you have ideas or suggestions about this, please comment [on Mastodon](https://scholar.social/@khinsen/109382151772277162)!
